<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[准备开发环境]]></title>
    <url>%2Fcategory%2F2018%2F10%2F31%2FDevelopment%20preparation.html</url>
    <content type="text"><![CDATA[开篇换电脑，记录下安装的应用，留作参考先将就电脑文件全部备份到百度云，然后开始安装系统、基本应用、开发环境 基本应用 WinRAR 、TIM、钉钉 、有道云笔记 、百度云 、火狐 / 谷歌浏览器 （谷歌插件）、everything 、输入法 、网易云音乐 开发环境 虚拟机： JDK服务器： Tomcat 、nginx数据库：Oracle 、mysql 、Oracle Windows安装SVN：TortoiseSVNGit：git 、 git廖雪峰教程接口调试：Postman数据库连接：navicat_premiumETL工具： kettle 开发工具： IntelliJ IDEA远程连接：SecureCRT/SecureFX反编译：jd-gui应用容器：Dockor持续集成：Jenkins文本编辑：notepad思维导图：Xmind虚拟化：VMware 、CentOS7Python : Python 、Python廖雪峰教程Node.js：Node.js正则：regexbuddy [Create：2018年8月14日]]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Hbase HA高可用集群搭建]]></title>
    <url>%2Fcategory%2F2018%2F10%2F31%2FHadoop%20Hbase%20HA%20distributed%20cluster%20construction.html</url>
    <content type="text"><![CDATA[基础环境准备根据前面hadoop集群搭建、hbase集群搭建添加外部zookeeper集群 下载zookeeper： zookeeper-3.4.13 zookeeper安装 1、下载及安装解压到/home/zookeeper/目录下：1tar -zxvf zookeeper-3.4.13.tar.gz -C /home/zookeeper/ 2、拷贝 zoo_sample.cfg进入zookeeper的conf目录，拷贝zoo_sample.cfg并重命名为zoo.cfg ：12cd zookeeper-3.4.13/conf/cp zoo_sample.cfg zoo.cfg 3、修改 zoo.cfg1vi zoo.cfg 修改如下，若原文件没有dataDir则直接添加：12345dataDir=/home/zookeeper/zookeeper-3.4.13/data/zkData//在最后添加，指定zookeeper集群主机及端口，机器数必须为奇数server.1=hadoop-master:2888:3888server.2=hadoop-slave1:2888:3888server.3=hadoop-slave2:2888:3888 4、创建并编辑myid//在zookeeper根目录下创建zoo.cfg中配置的目录1234567mkdir data/zkData/ -p//创建并编辑文件vi myid//输入1，即表示当前机器为在zoo.cfg中指定的server.11//保存退出:wq 5、拷贝zookeeper到其他机器上述操作是在hadoop-master机器上进行的，要将zookeeper拷贝到其他zookeeper集群机器上：123cd /home/zookeeperscp -r zookeeper-3.4.13/ hadoop-slave1:/home/zookeeper/scp -r zookeeper-3.4.13/ hadoop-slave2:/home/zookeeper/ 6、修改其他机器的myid文件myid文件是作为当前机器在zookeeper集群的标识，这些标识在zoo.cfg文件中已经配置好了，但是之前在hadoop-master这台机器上配置的myid为1，所以还需要修改其他机器的myid文件：1234//在hadoop-slave1机器上echo 2 &gt; /home/zookeeper/zookeeper-3.4.13/data/zkData/myid//在hadoop-slave2机器上echo 3 &gt; /home/zookeeper/zookeeper-3.4.13/data/zkData/myid 7、配置环境变量 vim /etc/profile1234添加：export ZOOKEEPER_HOME=/home/zookeeper/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/bin其它服务器同样配置 配置生效 source /etc/profile8、启动zookeeper集群123456789101112cd zookeeper-3.4.11/bin///分别在master188、master189、slave190上启动/home/zookeeper/zookeeper-3.4.13/bin/zkServer.sh start//查看状态/home/zookeeper/zookeeper-3.4.13/bin/zkServer.sh status三台机器的zookeeper状态必须只有一个leader，其他都是follower。//查看进程，若有QuorumpeerMain，则启动成功jps//停止/home/zookeeper/zookeeper-3.4.13/bin/zkServer.sh stop hadoop添加zookeeper 1、配置core-site.xml12345678910111213141516添加： &lt;!-- 指定ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点--&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop-master:2181,hadoop-slave1:2181,hadoop-slave2:2181&lt;/value&gt; &lt;/property&gt;修改： &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;/property&gt; 为： &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1&lt;/value&gt; &lt;/property&gt; 2、配置hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495&lt;configuration&gt; &lt;!-- 指定副本数，不能超过机器节点数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 为namenode集群定义一个services name --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;hadoop-master,hadoop-slave1&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为hadoop-master的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.hadoop-master&lt;/name&gt; &lt;value&gt;hadoop-master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为hadoop-slave1的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.hadoop-slave1&lt;/name&gt; &lt;value&gt;hadoop-slave1:9000&lt;/value&gt; &lt;/property&gt; &lt;!--名为hadoop-master的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.hadoop-master&lt;/name&gt; &lt;value&gt;hadoop-master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为hadoop-slave1的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.hadoop-slave1&lt;/name&gt; &lt;value&gt;hadoop-slave1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop-master:8485;hadoop-slave1:8485;hadoop-slave2:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled.ns1&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- journalnode 上用于存放edits日志的目录 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/tmp/data/dfs/journalnode&lt;/value&gt; &lt;/property&gt; &lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- connect-timeout超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3、配置 mapred-site.xml12345取消： &lt;!-- &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;--&gt; 4、配置 yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;configuration&gt; &lt;!-- 启用HA高可用性 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定resourcemanager的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm1的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm2的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop-slave1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定当前机器hadoop-master作为rm1 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper集群机器 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop-master:2181,hadoop-slave1:2181,hadoop-slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5、vi slaves123hadoop-masterhadoop-slave1hadoop-slave2 拷贝hadoop到其他机器 1、拷贝12scp -r /home/hadoop/hadoop hadoop-slave1:/home/hadoop/scp -r /home/hadoop/hadoop hadoop-slave2:/home/hadoop/ 2、修改yarn-site.xml在hadoop-slave1机器，即ResourceManager备用主节点上修改如下属性，表示当前机器作为rm2:：1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt;&lt;/property&gt; 同时删除hadoop-slave2机器上的该属性对，因为hadoop-slave2机器并不作为ResourceManager。 ####启动Hadoop1、启动zookeeper1/home/zookeeper/zookeeper-3.4.13/bin/zkServer.sh start 2、启动所有Journalnode1/home/hadoop/hadoop/sbin/hadoop-daemon.sh start journalnode 3、格式化master namenode（这里直接复制会有问题，最好手动输入）（第一次启动操作）12345678910/home/hadoop/hadoop/bin/hdfs namenode –format#启动 master namenode /home/hadoop/hadoop/sbin/hadoop-daemon.sh start namenode#master2上同步master namenode元数据 bin/hdfs namenode -bootstrapStandby#格式化 zk（在hadoop-master即可）（这里直接复杂会有问题，最好手动输入）/home/hadoop/hadoop/bin/hdfs zkfc –formatZK 4、启动HDFS、YARN、ZookeeperFailoverController12345/home/hadoop/hadoop/sbin/start-dfs.sh//jps验证，显示NameNode和DataNode/home/hadoop/hadoop/sbin/start-yarn.sh//jps 验证，显示ResourceManager和NodeManager 4、启动resourcemanager（hadoop-master、hadoop-slave1）1/home/hadoop/hadoop/sbin/yarn-daemon.sh start resourcemanager 5、启动zkfc来监控NN状态（在hadoop-master、hadoop-slave1）1/home/hadoop/hadoop/sbin/hadoop-daemon.sh start zkfc 启动命令：1234567#hadoop-master/home/hadoop/hadoop/sbin/start-all.sh/home/hadoop/hadoop/sbin/hadoop-daemon.sh start zkfc#hadoop-slave1/home/hadoop/hadoop/sbin/yarn-daemon.sh start resourcemanager/home/hadoop/hadoop/sbin/hadoop-daemon.sh start zkfc 停止命令：1234567#hadoop-master/home/hadoop/hadoop/sbin/stop-all.sh/home/hadoop/hadoop/sbin/hadoop-daemon.sh stop zkfc#hadoop-slave1/home/hadoop/hadoop/sbin/yarn-daemon.sh stop resourcemanager/home/hadoop/hadoop/sbin/hadoop-daemon.sh stop zkfc 启动所有进程显示： 错误处理：1、NameNode is not formatted1234原因: Path /home/hadoop/hadoop/hdfs/name should be specified as a URI in configuration files.方法:把dfs.namenode.name.dir、dfs.datanode.data.dir的原路径格式如/usr/mywind/name改成file:/usr/mywind/name，即使用完全路径。还有个原因：格式化命令复制进去运行报错，手动输入正常 测试wordcount程序测试，在本地创建一个测试文件，并上传到hdfs上12345678910111213#https:// 为下面文字加颜色https://#创建一个测试文件 vim test.txt #上传到hdfs上hadoop fs -put test.txt /input#查询hdfs上面是否存在input文件hadoop fs -ls /input#计算 hadoop jar hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar wordcount /input /output1#查看输出结果hadoop fs -cat /output1/part* Hbase安装配置进入/home/hbase/hbase/conf/目录，修改配置文件：1、配置 hbase-env.sh12345678//配置JDKexport JAVA_HOME=/usr/java///保存pid文件export HBASE_PID_DIR=/home/hbase/hbase/data/hbase/pids//修改HBASE_MANAGES_ZK，禁用HBase自带的Zookeeper，因为我们是使用独立的Zookeeperexport HBASE_MANAGES_ZK=false 2、配置 hbase-site.xml12345678910111213141516171819202122232425262728293031323334353637&lt;configuration&gt; &lt;!-- 设置HRegionServers共享目录，请加上端口号 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master188:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HMaster主机 --&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hdfs://master188:60000&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Zookeeper集群位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop-master:2181,hadoop-slave1:2181,hadoop-slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定独立Zookeeper安装路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/zookeeper/zookeeper-3.4.13&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ZooKeeper集群端口 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）vi regionservers修改regionservers文件，因为当前是使用独立的Zookeeper集群，所以要指定RegionServers所在机器：123hadoop-masterhadoop-slave1hadoop-slave2 4）创建pid文件保存目录在/home/hbase/hbase/目录下：1mkdir data/hbase/pids -p 3、拷贝HBase到其他机器12scp -r /home/hbase/hbase/ hadoop-slave1:/home/hadoop/scp -r /home/hbase/hbase/ hadoop-slave2:/home/hadoop/ 4、启动HBase在主节点上启动HBase（主节点指NameNode状态为active的节点，非指文中的机器声明）：1/home/hbase/hbase/bin/start-hbase.sh 5、查看HMaster、Regionserver进程是否启动123456jps注意：此时Hadoop集群应处于启动状态，并且是在主节点执行start-hbase.sh启动HBase集群，否则HMaster进程将在启动几秒后消失，而备用的HMaster进程需要在备用主节点单独启动，命令是：./hbase-daemon.sh start master。在备用主节点启动HMaster进程，作为备用HMaster：/home/hbase/hbase/bin/hbase-daemon.sh start master 5、HA高可用测试12345678910111213在浏览器中输入 ip:16010 ，查看主节点和备用主节点上的HMaster的状态，在备用主节点的web界面中，可以看到“Current Active Master: master188”，表示当前HBase主节点是master188机器；主节点---&gt;备用主节点这里的主节点指使用start-hbase.sh命令启动HBase集群的机器kill掉主节点的HMaster进程，在浏览器中查看备用主节点的HBase是否切换为active；若上述操作成功，则在主节点启动被杀死的HMaster进程：/home/hbase/hbase/bin/hbase-daemon.sh start master然后，kill掉备用主节点的HMaster进程，在浏览器中查看主节点的HBase是否切换为active，若操作成功，则HBase高可用集群搭建完成； 6、HBase基本操作123456789101112131415161718192021222324252627282930313233343536373839//启动HBase[root@vnet ~] start-hbase.sh//进入HBase Shell[root@vnet ~] hbase shell//查看当前HBase有哪些表hbase(main):&gt; list//创建表t_user，cf1和cf2是列族，列族一般不超过3个hbase(main):&gt; create 't_user','cf1','cf2'//获得表t_user的描述信息hbase(main):&gt; describe 't_user'//禁用表hbase(main):&gt; disable 't_user'//删除表，删除表之前要先把表禁用掉hbase(main):&gt; drop 't_user'//查询表是否存在hbase(main):&gt; exists 't_user'//查看全表数据hbase(main):&gt; scan 't_user'//插入数据，分别是表名、key、列（列族：具体列）、值。HBase是面向列的数据库，列可无限扩充hbase(main):&gt; put 't_user' ,'001','cf1:name','chenxj'hbase(main):&gt; put 't_user' ,'001','cf1:age','18'hbase(main):&gt; put 't_user' ,'001','cf2:sex','man'hbase(main):&gt; put 't_user' ,'002','cf1:name','chenxj'hbase(main):&gt; put 't_user' ,'002','cf1:address','fuzhou'hbase(main):&gt; put 't_user' ,'002','cf2:sex','man'//获取数据，可根据key、key和列族等进行查询hbase(main):&gt; get 't_user','001'hbase(main):&gt; get 't_user','002','cf1'hbase(main):&gt; get 't_user','001','cf1:age' 7、集群启动结果Hadoop + Zookeeper + HBase 高可用集群启动后，进程状态如下： 描述 hadoop-master hadoop-slave1 hadoop-slave2 HDFS主 NameNode NameNode HDFS从 DataNode DataNode DataNode YARN主 ResourceManager ResourceManager YARN从 NodeManager NodeManager NodeManager HBase主 HMaster HMaster HBase从 HRegionServer HRegionServer HRegionServer Zookeeper独立进程 QuorumPeerMain QuorumPeerMain QuorumPeerMain NameNodes数据同步 JournalNode JournalNode JournalNode 主备故障切换 DFSZKFailoverController DFSZKFailoverController 总结需要注意的地方： 1）备用节点上的NameNode、ResourceManager、HMaster均需单独启动；123hadoop-daemon.sh start namenodeyarn-daemon.sh start resourcemanagerhbase-daemon.sh start master 2）可以使用-forcemanual参数强制切换主节点与备用主节点，但强制切换后集群的自动故障转移将会失效，需要重新格式化zkfc：hdfs zdfc -formatZK;123（这个没有测试）hdfs haadmin -transitionToActive/transitionToStandby -forcemanual hadoop-slave1yarn rmadmin -transitionToActive/transitionToStandby -forcemanual rm2 3）在备用主节点同步主节点的元数据时，主节点的HDFS必须已经启动； 4）无法查看standby状态的节点上的hdfs； 5）格式化namenode时要先启动各个JournalNode机器上的journalnode进程：否则会报journalnode拒绝连接错误1hadoop-daemon.sh start journalnode； 6）若遇到问题，可以先考虑是哪个组件出现问题，然后查看该组件或与该组件相关的组件的日志信息；若各组件web页面无法访问，或存在其他连接问题，可以从「防火墙是否关闭」、「端口是否被占用」、「SSH」、「集群机器是否处于同一网段」内等角度考虑 参考： Hadoop HA高可用集群搭建（Hadoop+Zookeeper+HBase） [Create：2018年8月23日]]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hbase</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop分布式集群搭建]]></title>
    <url>%2Fcategory%2F2018%2F10%2F31%2FHadoop%20distributed%20cluster%20construction.html</url>
    <content type="text"><![CDATA[基础环境准备 1、软件环境 centos 6.5 三台服务器分配的IP地址：8/9/10jdk1.8hadoop使用2.7.4版本 2、host配置和主机名（三台） 修改四台服务器的hosts文件vim /etc/hosts 192.168.0.8 hadoop-master 192.168.0.9 hadoop-slave1 192.168.0.10 hadoop-slave2 分别修改服务器的主机名:HOSTNAME，master为例说明vi /etc/sysconfig/network HOSTNAME=hadoop-master执行reboot后生效，完成之后依次修改其它salve服务器为： hadoop-slave1~2。 3、服务器安装jdk（三台）建议使用yum安装jdk,也可以自行下载安装我是下载了1.8) yum -y install java-1.7.0-openjdk*下载的通过ssh复制到服务器 配置环境变量，修改配置文件vim /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0_152export PATH= \$JAVA_HOME/bin:\$PATHexport CLASSPATH=.:\$JAVA_HOME/lib/dt.jar:\$JAVA_HOME/lib/tools.jar 使用souce命令让立刻生效 source /etc/profile ###免密登陆1、首先关闭四台服务器的防火墙和SELINUX 查看防火墙状态 service iptables status 关闭防火墙 service iptables stopchkconfig iptables off 关闭SELINUX后，需要重启服务器 – 关闭SELINUX# vim /etc/selinux/config– 注释掉#SELINUX=enforcing#SELINUXTYPE=targeted– 添加SELINUX=disabled 2、免密码登录本机下面以配置hadoop-master本机无密码登录为例进行讲解，用户需参照下面步骤完成h-salve1~2三台子节点机器的本机无密码登录1） 生产秘钥 ssh-keygen -t rsa 2）将公钥追加到”authorized_keys”文件 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 3）赋予权限 chmod 600 .ssh/authorized_keys 4）验证本机能无密码访问 ssh hadoop-master最后，依次配置h-salve1~2无密码访问 3、hadoop-master本机无密码登录hadoop-slave1、hadoop-slave2以hadoop-master无密码登录hadoop-slave1为例讲解： 1）登录hadoop-slave1 ，复制hadoop-master服务器的公钥”id_rsa.pub”到hadoop-slave1服务器的”root”目录下。 scp root@hadoop-master:/root/.ssh/id_rsa.pub /root/ 2）将hadoop-master的公钥（id_rsa.pub）追加到hadoop-slave1的authorized_keys中 cat id_rsa.pub &gt;&gt; .ssh/authorized_keysrm -rf id_rsa.pub 3）在 hadoop-master上面测试 ssh hadoop-slave1 4、下面以hadoop-slave1无密码登录hadoop-master为例进行讲解，用户需参照下面步骤完成hadoop-slave2无密码登录hadoop-master。 1）登录hadoop-master，复制hadoop-slave1服务器的公钥”id_rsa.pub”到hadoop-master服务器的”/root/”目录下。 scp root@hadoop-slave1:/root/.ssh/id_rsa.pub /root/ 2）将hadoop-slave1的公钥（id_rsa.pub）追加到hadoop-master的authorized_keys中。 cat id_rsa.pub &gt;&gt; .ssh/authorized_keysrm -rf id_rsa.pub //删除id_rsa.pub 3）在 hadoop-slave1上面测试 ssh hadoop-master依次配置 hadoop-slave2 到此主从的无密登录已经完成了 ####Hadoop环境搭建配置hadoop-master的hadoop环境1、hadoop-master上 解压缩安装包及创建基本目录 #下载 (我已经下好2.7.4版本)wget http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz#解压tar -xzvf hadoop-2.7.3.tar.gz -C /usr/local#重命名mv hadoop-2.7.3 hadoop 2、 配置hadoop-master的hadoop环境变量 1）配置环境变量，修改配置文件vi /etc/profile export HADOOP_HOME=/home/hadoop/hadoopexport PATH=\$PATH:\$HADOOP_HOME/bin 使得hadoop命令在当前终端立即生效 source /etc/profile 下面配置，文件都在：/home/hadoop/hadoop/etc/hadoop路径下 2、配置core-site.xml 修改Hadoop核心配置文件/home/hadoop/hadoop/etc/hadoop/core-site.xml，通过fs.default.name指定NameNode的IP地址和端口号，通过hadoop.tmp.dir指定hadoop数据存储的临时文件夹。1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 特别注意：如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被删除，必须重新执行format才行，否则会出错。 3、配置hdfs-site.xml： 修改HDFS核心配置文件/usr/local/hadoop/etc/hadoop/hdfs-site.xml，通过dfs.replication指定HDFS的备份因子为3，通过dfs.name.dir指定namenode节点的文件存储目录，通过dfs.data.dir指定datanode节点的文件存储目录。1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4、配置mapred-site.xml 拷贝mapred-site.xml.template为mapred-site.xml，在进行修改 cp /home/hadoop/hadoop/etc/hadoop/mapred-site.xml.template /home/hadoop/hadoop/etc/hadoop/mapred-site.xmlvim /home/hadoop/hadoop/etc/hadoop/mapred-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5、配置yarn-site.xml后面两个property为2.7.4 nodemanager不启动添加，解决内存太小问题1234567891011121314151617181920&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; &lt;!-- 启动nodemanager value 为cpu核数 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep "cpu cores"| uniq --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6、配置masters文件 修改/home/hadoop/hadoop/etc/hadoop/masters文件，该文件指定namenode节点所在的服务器机器。删除localhost，添加namenode节点的主机名hadoop-master；不建议使用IP地址，因为IP地址可能会变化，但是主机名一般不会变化。 vi /home/hadoop/hadoop/etc/hadoop/masters## 内容hadoop-master 7、修改hadoop-env.sh vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh## 配置项export JAVA_HOME=/usr/java/jdk1.8.0_152 8、配置slaves文件（Master主机特有） 修改/home/hadoop/hadoop/etc/hadoop/slaves文件，该文件指定哪些服务器节点是datanode节点。删除locahost，添加所有datanode节点的主机名，如下所示。 vi /home/hadoop/hadoop/etc/hadoop/slaves## 内容hadoop-slave1hadoop-slave2hadoop-slave3 配置hadoop-slave的hadoop环境 下面以配置hadoop-slave1的hadoop为例进行演示，用户需参照以下步骤完成其他hadoop-slave2~3服务器的配置。 1）复制hadoop到hadoop-slave1节点 scp -r /home/hadoop/hadoop hadoop-slave1:/home/hadoop/ 登录hadoop-slave1服务器，删除slaves内容 rm -rf /home/hadoop/hadoop/etc/hadoop/slaves 2）配置环境变量 vi /etc/profile## 内容export HADOOP_HOME= /home/hadoop/hadoopexport PATH=\$PATH:\$HADOOP_HOME/bin 使得hadoop命令在当前终端立即生效； source /etc/profile期间报了一个错误-bash: export: ` /home/hadoop/hadoop’: not a valid identifier原因是/home前面多个空格 依次配置其它slave服务 ###启动集群1、格式化HDFS文件系统 进入master的~/hadoop目录，执行以下操作 bin/hadoop namenode -format 格式化namenode，第一次启动服务前执行的操作，以后不需要执行。 2、然后启动hadoop： sbin/start-all.sh 3、使用jps命令查看运行情况 #master 执行 jps查看运行情况25928 SecondaryNameNode25742 NameNode26387 Jps26078 ResourceManager #slave 执行 jps查看运行情况24002 NodeManager23899 DataNode24179 Jps 4、命令查看Hadoop集群的状态 通过简单的jps命令虽然可以查看HDFS文件管理系统、MapReduce服务是否启动成功，但是无法查看到Hadoop整个集群的运行状态。我们可以通过hadoop dfsadmin -report进行查看。用该命令可以快速定位出哪些节点挂掉了，HDFS的容量以及使用了多少，以及每个节点的硬盘使用情况。 hadoop dfsadmin -report 输出结果： Configured Capacity: 50108030976 (46.67 GB)Present Capacity: 41877471232 (39.00 GB)DFS Remaining: 41877385216 (39.00 GB)DFS Used: 86016 (84 KB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0…… 5、hadoop 重启 sbin/stop-all.shsbin/start-all.sh 参考：纯洁的微笑 :hadoop分布式集群搭建 [Create：2018年8月22日]]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase分布式集群搭建]]></title>
    <url>%2Fcategory%2F2018%2F10%2F31%2FHbase%20distributed%20cluster%20construction.html</url>
    <content type="text"><![CDATA[hbase依赖于hadoop环境，搭建habase之前首先需要搭建好hadoop的完全集群环境，因此看这篇文章之前需要先看我的上一篇文章：hadoop集群搭建 环境准备 hbase软件包: http://mirror.bit.edu.cn/apache/hbase/1.3.1/hbase-1.3.1-bin.tar.gz 完成hadoop集群环境搭建 安装hbase1、首先在hadoop-master安装配置好之后，在复制到从节点(我使用的版本是1.2.6)12345wget http://mirror.bit.edu.cn/apache/hbase/1.3.1/hbase-1.3.1-bin.tar.gz#解压tar -xzvf hbase-1.3.1-bin.tar.gz -C /usr/local/#重命名 mv hbase-1.3.1 hbase 2、配置环境变量1234编辑： vim /etc/profileexport HBASE_HOME=/home/hbase/hbaseexport PATH=$HBASE_HOME/bin:$PATH生效： source /etc/profile 3、修改系统变量ulimit ulimit -n 10240 配置文件 hbase 相关的配置主要包括hbase-env.sh、hbase-site.xml、regionservers三个文件，都在 /usr/local/hbase/conf目录下面： 1、配置hbase-env.sh12345678910vim hbase-env.sh#内容export JAVA_HOME=/usr/java/jdk1.8.0_152export HBASE_CLASSPATH=/home/hbase/hbase/conf# 此配置信息，设置由hbase自己管理zookeeper，不需要单独的zookeeper。export HBASE_MANAGES_ZK=trueexport HBASE_HOME=/home/hbase/hbaseexport HADOOP_HOME=/home/hadoop/hadoop#Hbase日志目录export HBASE_LOG_DIR=/home/hbase/hbase/logs 2、配置 hbase-site.xml123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop-master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hadoop-master:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop-master,hadoop-slave1,hadoop-slave2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3、配置regionservers123vim /usr/local/hbase/conf/regionservershadoop-slave1hadoop-slave2 4、 复制hbase到从节点中12scp -r /home/hbase/hbase hadoop-slave1:/home/hbase/hbase/scp -r/home/hbase/hbase hadoop-slave2:/home/hbase/hbase/ 启动hbase 启动仅在master节点上执行即可 ~/hbase/bin/start-hbase.sh启动后，master上进程和slave进程列表 master中的信息1234567[hadoop@master ~]$ jps6225 Jps2897 SecondaryNameNode # hadoop进程2710 NameNode # hadoop master进程3035 ResourceManager # hadoop进程5471 HMaster # hbase master进程2543 HQuorumPeer # zookeeper进程 salve中的信息12345[hadoop@slave1 ~]$ jps4689 Jps2533 HQuorumPeer # zookeeper进程2589 DataNode # hadoop slave进程4143 HRegionServer # hbase slave进程 因为hbase依赖于hadoop，因此启动和停止都是需要按照顺序进行 如果安装了独立的zookeeper12启动顺序: hadoop-&gt; zookeeper-&gt; hbase停止顺序：hbase-&gt; zookeeper-&gt; hadoop 使用自带的zookeeper12启动顺序: hadoop-&gt; hbase停止顺序：hbase-&gt; hadoop 重启hbase1234/home/hbase/hbase/bin/stop-hbase.sh/home/hadoop/hadoop/sbin/stop-all.sh /home/hadoop/hadoop/sbin/start-all.sh /home/hbase/hbase/bin/start-hbase.sh hbase UI界面 错误处理 1、启动hbase输出ignoring option PermSize=128m; support was removed in 8.0告警信息解决办法：由于JDK使用的是jdk1.8.0_65hbase-env.sh注释掉以下：123# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m"export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m" 2、hregionserver没有启动12345678910原因：在slave1、slave2主机中查看CST时间 [grid@slave1 bin]$ date 2018年 08月 22日 星期三 18:05:23 CST在master主机中查看CST时间 [grid@master bin]$ date 2018年 08月 22日 星期三 18:00:35 CST没错主从节点的系统日期是不一样的。 解决方法：把master主机的时间设置成和slave主机时间一致 [root@master bin]# date -s 18:06:002018年 08月 22日 星期三 18:06:00 CST 第二个原因，可以修改hbase默认的最大链接时间长一些。HBase配置文件hbase-siter.xml中添加连接时长的属性1234&lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;120000&lt;/value&gt; &lt;/property&gt; 参考:纯洁的微笑：hbase分布式集群搭建 [Create：2018年8月22日]]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 集群搭建]]></title>
    <url>%2Fcategory%2F2018%2F10%2F31%2FKafka%20distributed%20cluster%20construction.html</url>
    <content type="text"><![CDATA[基础环境 jdk 1.8zookeeper-3.4.13kafka_2.11-0.11.0.0 1、设置 host、安装 jdk 和免密登录 参照：hadoop 集群搭建 2、安装 zookeeper 集群 参照：Hadoop、Hbase HA 高可用集群搭建 zookeeper 启停脚本: ./start.sh start 123456789101112131415161718#!/bin/bash# 启动 ./start.sh start # 停止 ./start.sh stop#参数传递usage="Usage: $0 (start|stop|status)"if [ $# -lt 1 ]; then echo $usage exit 1fibehave=$1echo "$behave zkServer cluster"#主机名称for i in 1 2 3do#使用ssh进行启动ssh kafka$i "/home/zookeeper/zookeeper-3.4.13/bin/zkServer.sh $behave"doneexit 0 3、安装 kafka 集群 下载 kafka下载地址 下载，我这里选择2.11-0.11.0.0版本，通过ssh上传到服务器 解压 tar -zxvf kafka_2.11-0.11.0.0.tgz 设置环境变量 vim /etc/profile123export KAFKA_HOME=/home/kafka/kafka_2.11-0.11.0.0export PATH=$KAFKA_HOME/bin:$PATH#配置生效： source /etc/profile 修改配置文件进入到config目录 cd /home/kafka/kafka_2.11-0.11.0.0/config/ 123456789101112131415161718broker.id=0 #当前机器在集群中的唯一标识，和zookeeper的myid性质一样,每台服务器的broker.id都不能相同port=19092 #当前kafka对外提供服务的端口默认是9092host.name=192.168.7.100 #这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。num.network.threads=3 #这个是borker进行网络处理的线程数num.io.threads=8 #这个是borker进行I/O处理的线程数log.dirs=/opt/kafka/kafkalogs/ #消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个socket.send.buffer.bytes=102400 #发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能socket.receive.buffer.bytes=102400 #kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘socket.request.max.bytes=104857600 #这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小num.partitions=1 #默认的分区数，一个topic默认1个分区数log.retention.hours=168 #默认消息的最大持久化时间，168小时，7天message.max.byte=5242880 #消息保存的最大值5Mdefault.replication.factor=2 #kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务replica.fetch.max.bytes=5242880 #取消息的最大直接数log.segment.bytes=1073741824 #这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件log.retention.check.interval.ms=300000 #每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除log.cleaner.enable=false #是否启用log压缩，一般不用启用，启用的话可以提高性能zookeeper.connect=192.168.7.100:12181,192.168.7.101:12181,192.168.7.107:1218 #设置zookeeper的连接端口 复制到其他服务器 123scp -r /home/kafka/kafka_2.11-0.11.0.0 kafka2: /home/kafkascp -r /home/kafka/kafka_2.11-0.11.0.0 kafka3: /home/kafka#修改broker.id、host.name 4、启动Kafka集群并测试 启动服务 123456#1.启动zookeeper集群（3台都需要启动）cd /home/zookeeper/zookeeper-3.4.13/bin./zkServer.sh start#从后台启动Kafka集群（3台都需要启动） cd /home/kafka/kafka_2.11-0.11.0.0/bin #进入到kafka的bin目录 ./kafka-server-start.sh ../config/server.properties 检查服务是否启动 1234#执行命令jps20348 Jps 4233 QuorumPeerMain 18991 Kafka 创建Topic来验证是否创建成功 12345678910111213#创建Topic./kafka-topics.sh --create --zookeeper 192.168.7.100:12181 --replication-factor 2 --partitions 1 --topic shuaige#解释--replication-factor 2 #复制两份--partitions 1 #创建1个分区--topic #主题为shuaige'''在一台服务器上创建一个发布者'''#创建一个broker，发布者./kafka-console-producer.sh --broker-list 192.168.7.100:19092 --topic shuaige'''在一台服务器上创建一个订阅者'''./kafka-console-consumer.sh --zookeeper localhost:12181 --topic shuaige --from-beginning kafka 启停脚本：注：kafka停止脚本要调用kafka官方的kafka-server-stop.sh脚本。 但是官方的kafka-server-stop.sh 脚本是优点问题，不会真正的停止kafka。所以要先修改kafka-server-stop.sh 123PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')修改为： PIDS=$(jps -lm | grep -i 'kafka\.Kafka' | awk '&#123;print $1&#125;') 1234567891011121314151617#启动：#!/bin/bashbrokers="kafka1 kafka2 kafka3"kafka_home="/home/kafka/kafka_2.11-0.11.0.0"for i in $brokersdo echo "Starting kafka on $&#123;i&#125; ... " ssh $&#123;i&#125; "source /etc/profile; nohup sh $&#123;kafka_home&#125;/bin/kafka-server-start.sh $&#123;kafka_home&#125;/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;" if [[ $? -ne 0 ]]; then echo "Start kafka on $&#123;i&#125; is OK !" fidoneecho kafka kafka are started !exit 0 1234567891011121314151617#停止#!/bin/bash brokers="kafka1 kafka2 kafka3"kafka_home="/home/kafka/kafka_2.11-0.11.0.0"for i in $brokersdo echo "Stopping kafka on $&#123;i&#125; ..." ssh $&#123;i&#125; "source /etc/profile;bash $&#123;kafka_home&#125;/bin/kafka-server-stop.sh" if [[ $? -ne 0 ]]; then echo "Stopping $&#123;kafka_home&#125; on $&#123;i&#125; is down" fidoneecho all kafka are stopped !exit 0 更多请看官方文档：http://kafka.apache.org/documentation.html 测试（在发布者那里发布消息看看订阅者那里是否能正常收到~） yahoo kafka 管理平台 安装 kafka-manager下载编译好的，修改配置文件zookeeper参数，启动运行注：关闭重启时，要删除RUNNUING文件 5、 其他命令大部分命令可以去官方文档查看 查看topic 12./kafka-topics.sh --list --zookeeper localhost:12181#就会显示我们创建的所有topic 查看topic状态 123456/kafka-topics.sh --describe --zookeeper localhost:12181 --topic shuaige#下面是显示信息Topic:ssports PartitionCount:1 ReplicationFactor:2 Configs: Topic: shuaige Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1#分区为为1 复制因子为2 他的 shuaige的分区为0 #Replicas: 0,1 复制的为0，1 OK kafka集群搭建完毕 6、其他说明标注 日志说明 默认kafka的日志是保存在/opt/kafka/kafka_2.10-0.9.0.0/logs目录下的，这里说几个需要注意的日志1234server.log #kafka的运行日志state-change.log #kafka他是用zookeeper来保存状态，所以他可能会进行切换，切换的日志就保存在这里controller.log #kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller. 上面的大家你完成之后可以登录zk来查看zk的目录情况123456789101112131415161718192021222324252627282930313233343536373839404142#使用客户端进入zk./zkCli.sh -server 127.0.0.1:12181 #默认是不用加’-server‘参数的因为我们修改了他的端口#查看目录情况 执行“ls /”[zk: 127.0.0.1:12181(CONNECTED) 0] ls /#显示结果：[consumers, config, controller, isr_change_notification, admin, brokers, zookeeper, controller_epoch]'''上面的显示结果中：只有zookeeper是，zookeeper原生的，其他都是Kafka创建的'''#标注一个重要的[zk: 127.0.0.1:2181(CONNECTED) 1] get /brokers/ids/0&#123;"jmx_port":-1,"timestamp":"1456125963355","endpoints":["PLAINTEXT://192.168.7.100:19092"],"host":"192.168.7.100","version":2,"port":19092&#125;cZxid = 0x1000001c1ctime = Mon Feb 22 15:26:03 CST 2016mZxid = 0x1000001c1mtime = Mon Feb 22 15:26:03 CST 2016pZxid = 0x1000001c1cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x152e40aead20016dataLength = 139numChildren = 0[zk: 127.0.0.1:12181(CONNECTED) 2] #还有一个是查看partion[zk: 127.0.0.1:2181(CONNECTED) 7] get /brokers/topics/shuaige/partitions/0nullcZxid = 0x100000029ctime = Mon Feb 22 10:05:11 CST 2016mZxid = 0x100000029mtime = Mon Feb 22 10:05:11 CST 2016pZxid = 0x10000002acversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1[zk: 127.0.0.1:12181(CONNECTED) 8] [Create：2018年8月23日]]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启 Hexo 博客之旅]]></title>
    <url>%2Fcategory%2F2018%2F10%2F30%2FOpen%20the%20Hexo%20blog%20tour.html</url>
    <content type="text"><![CDATA[Hexo + github pages 搭建博客 ，速度快的飞起，直接按网上的步骤开始参考如下，可以直接按第二个去弄，主题是 next 使用 hexo+github pages 搭建博客：http://laijianfeng.org/2018/05/使用hexo-github-pages搭建博客/ 打造个性化博客（极致详细）：https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html 修改文章内链接样式｜hexo： https://blog.csdn.net/qw8880000/article/details/80235648 hexo 的 next 主题个性化教程：打造炫酷网站：https://blog.csdn.net/qq_33699981/article/details/72716951?utm_source=blogxgwz1 在 NexT 中使用 Valine 评论系统：https://reuixiy.github.io/technology/computer/computer-aided-art/2018/07/15/use-valine-in-theme-next.html 安装没什么问题，个性化优化过程有点繁琐 遇到问题有以下几点： 图片本地可以，上传到 github 除了主页，其他的跳转后显示有问题，图片路径修改为 github 的存储路径就好了，也可以用七牛云存储图片，但给的测试域名只有一个月有效期，长期的必须要备案的域名才行，后续在处理图片存储 页面 CSS 样式调整，比较费时，有前端的朋友找他们比较快速 用谷歌浏览器调试，内存飞速溢出，网站直接打不开，换火狐就好了（试了下 reuixiy 的博客，同样的问题，那就不是我安装的问题，后续有时间再看） 其他的问题在 reuixiy 的博客里都可以找到答案 YAMLException: end of the stream or a document separator is expected at line 2 好一番检查才发现第一行 — 变成 – 两个了 （特别注意符号，英文: 和空格） 置顶无效，查看~/blog/node_modules/hexo-generator-index-pin-top/lib/generator.js 发现改成strict 的又还原回top[Create：2018年10月30日] 话不多说，接下来就是开始我的博客之旅~~~]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开篇]]></title>
    <url>%2Fcategory%2F2018%2F10%2F29%2Fbegin.html</url>
    <content type="text"><![CDATA[一件事总要去做，最好就是现在，也许有点晚，总算开始了Just do it]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fcategory%2F2018%2F10%2F29%2Fhello-world.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
